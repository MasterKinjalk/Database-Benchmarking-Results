---
- name: Install and configure Spark on a cluster
  hosts: spark_cluster
  become: yes
  tasks:

    - name: Install OpenJDK 8
      ansible.builtin.yum:
        name: java-1.8.0-openjdk
        state: present
      tags: java-install

    - name: Set JAVA_HOME environment variable
      ansible.builtin.lineinfile:
        path: /etc/profile.d/java.sh
        line: 'export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))'
        create: yes
      tags: java-home

    - name: Push Spark
      ansible.builtin.copy:
        src: "/mnt/f/CS 511/Database Benchmarking/Spark Setup/spark-3.4.1-bin-hadoop3.tgz"
        dest: /tmp/spark-3.4.1-bin-hadoop3.tgz
      tags: spark-download

    - name: Extract Spark
      unarchive:
        src: "/tmp/spark-3.4.1-bin-hadoop3.tgz"
        dest: "/opt"
        creates: "/opt/spark-3.4.1-bin-hadoop3" # This ensures the task is idempotent
        remote_src: yes
      tags: spark-extract

    - name: Check if the target Spark directory exists
      stat:
        path: /opt/spark
      register: spark_dir

    - name: Remove the existing Spark directory if it exists
      file:
        path: /opt/spark
        state: absent
      when: spark_dir.stat.exists

    - name: Move Spark to the desired directory
      command:
        _raw_params: mv /opt/spark-3.4.1-bin-hadoop3 /opt/spark
      tags: spark-move

    - name: Set Spark environment variables
      blockinfile:
        path: "/etc/profile.d/spark.sh" # It's better to use a custom file in /etc/profile.d
        block: |
          export SPARK_HOME=/opt/spark
          export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
        create: yes
        owner: root
        group: root
        mode: '0644'
      tags: environment


    - name: Configure Spark for Main Node
      block:
        - file:
            path: "$SPARK_HOME/conf"
            state: directory
        - copy:
            src: "$SPARK_HOME/conf/spark-env.sh.template"
            dest: "$SPARK_HOME/conf/spark-env.sh"
            remote_src: yes
        - lineinfile:
            path: "$SPARK_HOME/conf/spark-env.sh"
            line: "SPARK_MASTER_HOST=main"
        - lineinfile:
            path: "$SPARK_HOME/conf/spark-env.sh"
            line: "SPARK_PUBLIC_DNS=main"
        - copy:
            src: "$SPARK_HOME/conf/spark-defaults.conf.template"
            dest: "$SPARK_HOME/conf/spark-defaults.conf"
            remote_src: yes
        - lineinfile:
            path: "$SPARK_HOME/conf/spark-defaults.conf"
            line: "spark.master spark://main:7077"
      when: "'main' in inventory_hostname"
      tags: main-config


    - name: Configure Spark for Worker Node
      block:
        - file:
            path: "$SPARK_HOME/conf"
            state: directory
        - copy:
            src: "$SPARK_HOME/conf/spark-env.sh.template"
            dest: "$SPARK_HOME/conf/spark-env.sh"
            remote_src: yes
        - lineinfile:
            path: "$SPARK_HOME/conf/spark-env.sh"
            line: "SPARK_MASTER_HOST=main"
        - shell:
            cmd: echo "SPARK_PUBLIC_DNS=$(hostname -f)" >> $SPARK_HOME/conf/spark-env.sh
      when: "'worker' in inventory_hostname"
      tags: worker-config

    - name: Set JAVA_HOME environment variable
      lineinfile:
        path: "/etc/profile.d/spark.sh"
        line: "export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))"
      tags: java-home

    - name: Remove the Spark archive
      file:
        path: /tmp/spark-3.4.1-bin-hadoop3.tgz
        state: absent
      tags: spark-remove
